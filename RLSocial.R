# this function simulates choices of an agent 
# who integrates social information in the prey selection task 
# the social information is delivered approximately every 30 seconds.

# inputs:
# beta_self : learning rate for self-generated outcomes
# beta_other : learning rate for outcomes generated by others
# tau :  temperature parameter in the soft-max function
# ctxRwd : reward for the context modualtion trials 
# rwd_ : reward sequences 

# outputs:
# tIdxInBlock [nTrialx1 int]: trial index within the block
# tIdxInChunk [nTrialx1 int]: trial index within the chunk
# rwd [nTrialx1 int]: trial-wise rewards
# trialEarnings [nTrialx1 int]: trial-wise payments
# requiredHt [nTrialx1 int]: time spent on each trial, 0 if forgo the choice and ht otherwise
# blockTime [nTrialx1 int]: elapased time since the beginning of the block
# reRate  [nTrialx1 real]: trial-wise estimates for the long-run reward rate
# delta [nTrialx1 real]: trial-wise prediction errors
# iniLongRunRate [1x1 real]: initial estimate for the long-run reward rate
# iniMinAcpRwd [1x1 real]: minimal reward accepted initially 
# iniNLeaveProb [1x1 real]: number of forgone prob rewards initially
# optimLongRunRate [1x1 real]: the optimal long-run reward rate 
# optimNLeaveProb [1x1 real]: number of forgone prob rewards optimally 
# optimMinAcpRwd [1x1 real]: minimal reward accepted under the optimal policy

RLSocial = function(beta_self, beta_other, tau, iniLongRunRate, ctxRwd, rwd_){
  # load expParas
  load("expParas.RData")
  
  # rewards constant
  rwds = c(probRwds, rep(ctxRwd, nCtx)) # all possible rewards, context rewards duplicated
  unqRwds = sort(unique(rwds)) # all possible rewards, unique values 
  
  # ctxRwd should not equal any possible reward in prob trials 
  if(any(ctxRwd == probRwds)){
    print("ctxRwd should not equal rewards in prob trials!")
    break
  }
  
  ################### optim analysis ################
  # calculate the optim long-run reward rates and the optim threshold 
  
  # initial policy
  iniMinAcpRwd = min(unqRwds[ unqRwds / ht >= iniLongRunRate]) # minimal reward accepted initially 
  iniNLeaveProb = sum(probRwds < iniMinAcpRwd) # number of forgone prob rewards initially
  
  # calculate the long-term reward rate for every possible decision threshold 
  # noticably, long-term reward rates are different from local reward rates
  # whereas local reward rate = reward / ht
  # and long-term reward rates = E[reward] / E[iti + ht]
  longRunRates = sapply(1 : length(unqRwds),
                        function(j) {
                          totalReward = sum(probRwds[probRwds >=  unqRwds[j]]) + 
                            ifelse(ctxRwd >= unqRwds[j], ctxRwd * nCtx, 0) 
                          totalTime = iti * nStim + 
                            ifelse(ctxRwd >=  unqRwds[j], ht * nCtx, 0) + 
                            sum(probRwds >=  unqRwds[j]) * ht
                          totalReward / totalTime
                        })
  optimLongRunRate = max(longRunRates)
  optimMinAcpRwd = min(unqRwds[unqRwds / ht >= optimLongRunRate])
  optimNLeaveProb = sum(probRwds < optimMinAcpRwd)
  
  ############################# model ################################
  # initialize variables 
  reRate = iniLongRunRate # initial estimate of the long-run reward rate
  blockTime = 0 # elapsedTime since the beginning of the block
  elapsedTime = 0 # elapsed time since the last update f social information
  enctRwds = c() # encountered rewards since the last update f social information
  
  # initialize recording variables 
  chunkNum_ = rep(1 : nChunkMax, each = nStim)
  chunkNum_ = chunkNum_[1 : nTrialMax]
  tIdxInBlock_ = 1 : nTrialMax # trial indexs within this block
  tIdxInChunk_ = rep(1 : nStim, nChunkMax) # trial indexs within each chunk
  tIdxInChunk_ = tIdxInChunk_[1 : nTrialMax]
  requiredHt_ = rep(NA, length = nTrialMax) # variables to record spent time, if engage = ht otherwise = 0
  blockTime_ = vector(length = nTrialMax)
  reRate_ = vector(length = nTrialMax) # variable to record reRate
  trialEarnings_ = vector(length = nTrialMax) # variable to record trialEarnings
  delta_ = vector(length = nTrialMax) # diagnosis variable to record prediction errors
  
  # loop over trials
  i = 1
  while(blockTime < blockSec){
    # current rwd
    rwd = rwd_[i] 
    
    # make the action
    pAccept = 1 / (1 + exp( ((ht) * reRate - rwd) * tau)) 
    action = ifelse(runif(1) <= pAccept, "accept", "forgo") 
    trialEarnings = ifelse(action == "accept", rwd, 0)
    requiredHt = ifelse(action == "accept", ht, 0)
      
    # update reRate given the self-generated outcome
    delta  = trialEarnings - (requiredHt + iti) * reRate # we should one sec stop of the task
    reRate = reRate + beta_self * delta
    
    # update elapsedTime and enctRwds since the last update of social info
    elapsedTime = elapsedTime + iti + requiredHt 
    enctRwds = c(enctRwds, rwd)
    
    # if more than 15 seconds have elapsed since the last upate of
    # social info, provide a new update and reset elapsedTime, accEarnings, to 0
    if(elapsedTime >= 15){
      # the social information is the accumulated earnings of an optimal agent in the past 15 seconds
      # The optimal agent encounters the same reward sequence, as recorded in enctRwds, and accept 
      # any reward >= the optimal threshold. The optimal agent might undergo the reward sequence faster or
      # slower than the real player. If slower, it only obtains rewards collected within the around 15s
      # window. If faster, we assume it performs the optimal strategy in the rest of the time and earns 
      # the expected earnings under that strategy. 
      
      # earnings accumulated by the optimal player 
      optimRequiredHts = ifelse(enctRwds >= optimMinAcpRwd, ht, 0)
      accEarningsOther = sum(enctRwds[enctRwds >= optimMinAcpRwd &
                                        (cumsum(optimRequiredHts + iti) - iti) <= elapsedTime]) # minus iti for the last trial
      if(sum(optimRequiredHts + iti) < elapsedTime){
        accEarningsOther = accEarningsOther + optimLongRunRate * (elapsedTime - sum(optimRequiredHts +iti))
      }
      
      # update based social info
      deltaOther =  accEarningsOther - (elapsedTime *  reRate)
      reRate = reRate + beta_other * deltaOther
      
      # reset elapsedTime and enctRwds 
      elapsedTime = 0
      enctRwds = c()
    }
    
    # update blockTime
    blockTime = blockTime + iti + requiredHt
    
    # save
    if(blockTime <= blockSec){
      requiredHt_[i] = requiredHt
      delta_[i] = delta
      reRate_[i] = reRate
      trialEarnings_[i] = trialEarnings
      blockTime_[i] = blockTime
    }
    
    # update trial index
    i = i + 1
  }
  
  # num of actual trials
  nTrial = ifelse(blockTime == blockSec, i -1, i - 2)
  nChunk = ceiling(nTrial / nStim)
  
  # calculate how responses to prob stims change
  acceptMatrix = matrix(NA, nProb, nChunk)
  for(i in 1 : nProb){                                                                                                                                                                                                                                                                                                                                                                                                                                              
    for(j in 1 : nChunk){
      acceptMatrix[i, j] = requiredHt_[rwd_ == probRwds[i] & chunkNum_ == j] == ht
    }                                               
  }
  
  # map reRate and acceptMatrix to a standard time grid                                    
  tGrid = seq(0, blockSec, by = 2)  
  nT = length(tGrid)
  reRateOnGrid = vector(length = nT)
  for(i in 1 : nT){
    t = tGrid[i]
    if(t >= min(blockTime_[tIdxInBlock_ <= nTrial])){
      reRateOnGrid[i] = reRate_[max(which(blockTime_ <= t & tIdxInBlock_ <= nTrial))]
    }else{
      reRateOnGrid[i] = iniLongRunRate
    }
  }
  acceptMatrixOnGrid = matrix(NA, nrow = nProb, ncol = nT)
  endOfBlockTimes = blockTime_[tIdxInChunk_ == nStim & tIdxInBlock_ <= nTrial]
  for(i in 1 : nT){
    t = tGrid[i]
    if(t <= max(endOfBlockTimes)){
      acceptMatrixOnGrid[,i] = acceptMatrix[,min(which(endOfBlockTimes >= t))]
    }else{
      acceptMatrixOnGrid[,i] = acceptMatrix[,nChunk]
    }
  }
  
  # return outputs
  junk = data.frame(
    'tIdxInBlock' = tIdxInBlock_,
    'tIdxInChunk' = tIdxInChunk_,
    "rwd" = rwd_,
    "trialEarnings" = trialEarnings_,
    "requiredHt" = requiredHt_,
    "reRate" = reRate_,
    "delta" = delta_
  )
  junk = junk[1 : nTrial, ]
  outputs = list(
    "optimLongRunRate" = optimLongRunRate,
    "optimMinAcpRwd" = optimMinAcpRwd,
    "optimNLeaveProb" = optimNLeaveProb,
    "iniLongRunRate" =   iniLongRunRate,
    "iniMinAcpRwd" = iniMinAcpRwd,
    "iniNLeaveProb" = iniNLeaveProb,
    "acceptMatrix" = acceptMatrix,
    "acceptMatrixOnGrid" = acceptMatrixOnGrid,
    "reRateOnGrid" = reRateOnGrid 
  )
  outputs = c(junk, outputs)
  return(outputs)
}